With the advent of social media lots of people have found their voice, on one hand this has led to democratization and diversification of opinions on whole range of subjects (from politics to education to food), on the other hand it has also led to people abusing anyone who disagrees with them, at times even threatening others with physical harm.

But not all the statements are threatening; some are insulting and toxic but not threatening. Aim of this project is to build a multi-headed model (a statement can fall into multiple categories simultaneously) that is capable of detecting different types of toxicity in statements like insulting, obscene, threatening etc. This will hopefully help in making online discussions more productive and respectful.

Below is a description of project files:

1. This folder contains a sub-folder called 'code', this sub-folder contains six files, they are:
  A. Generic_Functions - Contains common functions used in other Python files.
  B. Exploratory_Data_Analysis - Computes distribution, plot and word clouds related to data.
  C. NB_LR - Code for establishing baseline.
  D. Logistic_Regression - Contains experiments of Logistic Regression with different problem transformation methods.
  E. Multinomial_Naive_Bayes - Contains experiments of Multinomial Naive Bayes with different problem transformation methods.
  F. Decision_Trees - Contains experiments of Decision Trees with different problem transformation methods.

2. This folder contains project report.

3. This folder contains a csv file, which contains the dataset used in this project.

Note : Please change the paths accordingly when running the code.
